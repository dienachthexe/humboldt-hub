name: Automagical Multi-Collection Harvest

on:
  workflow_dispatch: {}

jobs:
  harvest:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout Hub
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install deps
        run: |
          python -m pip install --upgrade pip
          pip install sickle

      - name: Clear Old Data
        run: |
          # This ensures we are NOT appending to the 102k record file
          rm -f master.csv 

      - name: Run Precision Harvest
        shell: bash
        run: |
          python <<'PYCODE'
          from sickle import Sickle
          import csv
          import xml.etree.ElementTree as ET
          import glob
          import sys

          CONFIG_PATH = 'repository_config.csv'
          OUTPUT_CSV  = 'master.csv'

          # This list is used for the "Aggressive" check (if it's in the name, skip it)
          blacklist_keywords = [
              'alra','apportionment','authorresources','booger','botany_jps',
              'cahssnews','capstone','careercurriculumconnections','ccv',
              'chanterelle','clubnews','comm300','conferences','contaminated_sites',
              'courageouscuentos','creative_pub','crp','csucompetitionvideos',
              'csuglobalaction','csuglobaljournal','culturaltimes','data','digitallab',
              'esmproject','etd','etd_slideshow','faculty','facultypub','fiction',
              'foodfutures','gradnews','gradslam','gspproject','h5ii','hb_infrastructure',
              'hb_sustainability','hcapc','herbarium_photos',
              'histpaper','hjm','hjsr','hjsrvideo','homepage_gallery','hsuslri',
              'hsuslri_geospatial','hsuslri_local','hsuslri_presentations','hsuslri_state',
              'hsuslri_student','humboldtgeographic','humnews','ideafest-events','ije',
              'inrsep_posters','internationalnews','interviews','isfsi','isfsi_geospatial',
              'isfsi_local','isfsi_presentations','isfsi_state','isfsi_student','journals',
              'jscc_conference','jscc_student','librarian','library_pub','librarypub',
              'math','monographs','msw','oceanogrpahypub','odeinews','oer',
              'oer_sustainability','open_ed','pelicanbayresearchjournal','personas',
              'physicssims','politics','polyarc','posters','pracademics','press',
              'projectrebound','projects','proposals','rectourism','reprint','rr','rrv',
              'rspnews','rwc','senior_cd','senior_comm','senior_crgs','senior_esm',
              'senior_ffrm','senior_hist','senior_math','senior_soc','sotl_ip','student',
              'studentcomm','studenthist','studentresearchbio','studentscholar',
              'sustainability','telonichernews','textbooks','theosprey','toyon',
              'toyonv','unconference','wildlife_posters','world','worlddahs',
              'worldwlc','wrrap','xavhumboldt','yesv','zerowaste',
              'studentnewspaper', 'ellenador'
          ]

          def safe_join(values):
              return ' ; '.join(v for v in values if v).strip()

          def get_marc(rec, tag, code):
              f = rec.find(f".//*[@tag='{tag}']")
              if f is None: return ''
              subs = [sf.text for sf in f.findall(f".//*[@code='{code}']") if sf.text]
              return ' '.join(subs).strip()

          all_rows = []
          seen_links = set() # PREVENTS DOUBLE HARVESTING OF SAME LINK
          header = ['Title','Description','Subject','Date','Collection','Institution','Thumbnail','Link']

          try:
              with open(CONFIG_PATH, mode='r', encoding='utf-8-sig') as f:
                  config_list = list(csv.DictReader(f))
          except Exception as e:
              print(f"Error reading config: {e}"); sys.exit(1)

          for repo in config_list:
              ptype = (repo.get('Platform_Type') or '').strip()
              base  = repo.get('Base_URL')
              prefix = repo.get('Prefix')

              if ptype == 'DigitalCommons':
                  print(f"Starting DigitalCommons harvest...")
                  try:
                      sickle = Sickle(base)
                      for s in sickle.ListSets():
                          raw_set = s.setSpec.lower()
                          # The Aggressive Guard
                          if any(kw in raw_set for kw in blacklist_keywords):
                              continue
                          
                          try:
                              records = sickle.ListRecords(metadataPrefix=prefix, set=s.setSpec, ignore_deleted=True)
                              for record in records:
                                  m = record.metadata
                                  link = next((u for u in m.get('identifier', []) if isinstance(u, str) and 'humboldt.edu' in u and '.jpg' not in u), '')
                                  
                                  if link and link not in seen_links:
                                      title = safe_join(m.get(repo['Title_Field'], []))
                                      desc = safe_join([u for u in m.get(repo['Description_Field'], []) if not (isinstance(u, str) and u.lower().endswith('.jpg'))])
                                      subj = safe_join(m.get(repo['Subject_Field'], []))
                                      date = safe_join(m.get(repo['Date_Field'], []))
                                      coll = safe_join(m.get(repo['Collection_Field'], []))
                                      thumb = next((u for u in m.get('description', []) if isinstance(u, str) and '.jpg' in u), 'https://img.icons8.com/color/96/document.png')
                                      
                                      all_rows.append([title, desc, subj, date, coll, repo['Institution_Name'], thumb, link])
                                      seen_links.add(link)
                          except: continue
                  except: pass

              elif ptype == 'CONTENTdm':
                  print("Starting CONTENTdm harvest...")
                  try:
                      sickle = Sickle(base)
                      records = sickle.ListRecords(metadataPrefix=prefix, set='palmquistyale', ignore_deleted=True)
                      for record in records:
                          sid = record.header.identifier.split('/')[-1]
                          link = repo['Link_Pattern'].format(setSpec='palmquistyale', id=sid)
                          
                          if link not in seen_links:
                              m = record.metadata
                              all_rows.append([
                                  safe_join(m.get(repo['Title_Field'], [])),
                                  safe_join([u for u in m.get(repo['Description_Field'], []) if not (isinstance(u, str) and u.lower().endswith('.jpg'))]),
                                  safe_join(m.get(repo['Subject_Field'], [])),
                                  safe_join(m.get(repo['Date_Field'], [])),
                                  safe_join(m.get(repo['Collection_Field'], [])),
                                  repo['Institution_Name'],
                                  repo['Thumbnail_Pattern'].format(setSpec='palmquistyale', id=sid),
                                  link
                              ])
                              seen_links.add(link)
                  except: pass

              elif 'Archives' in ptype:
                  print("Processing local Archives XML files...")
                  for xml_file in glob.glob('finding*.xml'):
                      try:
                          tree = ET.parse(xml_file)
                          root = tree.getroot()
                          marcs = root.findall('.//record') or root.findall('.//{http://www.loc.gov/MARC21/slim}record')
                          for record in marcs:
                              f001 = record.find(".//*[@tag='001']")
                              mms_id = f001.text if f001 is not None else ''
                              link = repo['Link_Pattern'].format(mms_id=mms_id) if mms_id else ''
                              
                              if link and link not in seen_links:
                                  f008 = record.find(".//*[@tag='008']")
                                  date = f008.text[7:11] if (f008 is not None and f008.text and len(f008.text) >= 11) else ''
                                  all_rows.append([
                                      get_marc(record, '245', 'a'),
                                      get_marc(record, '520', 'a'),
                                      get_marc(record, '650', 'a'),
                                      date,
                                      get_marc(record, '710', 'a'),
                                      repo['Institution_Name'],
                                      'https://img.icons8.com/color/96/folder-invoices--v1.png',
                                      link
                                  ])
                                  seen_links.add(link)
                      except: continue

          # Write mode 'w' ensures the file is wiped first
          with open(OUTPUT_CSV, 'w', newline='\n', encoding='utf-8') as f:
              writer = csv.writer(f)
              writer.writerow(header)
              writer.writerows(all_rows)
          print(f"Success! Final count: {len(all_rows)} UNIQUE records.")
          PYCODE

      - name: Force Overwrite Commit
        shell: bash
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add master.csv
          # We use --force to ensure the clean file replaces the bloated one
          git commit -m "Clean Slate Harvest: $(date)" || echo "No changes"
          git push --force